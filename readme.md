# Installation

* ROS Noetic (http://wiki.ros.org/noetic/Installation/Ubuntu)
* Gazebo Fortress v6.15.0 (https://gazebosim.org/docs/fortress/install_ubuntu)
* ros_bridge specifically using Noetic branch!!! (https://github.com/gazebosim/ros_gz/tree/noetic)
* teleop_twist_keyboard (http://wiki.ros.org/teleop_twist_keyboard)

Make sure you have created a catkin workspace (workspace can name can be something different than "catkin_ws")

```python
mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/
catkin_make
```

# How to run the simulation

Always make sure to source the setup.*sh

```python
cd ~/catkin_ws/
source devel/setup.bash
```

Always make sure to source the model files by running the set_hospital_export.sh by:

```
cd ~/catkin_ws/
source src/semantic_gazebo_simulation/scripts/set_hospital_export.sh
```

After sourcing run the launch file

```
roslaunch semantic_gazebo_simulation semantic_sim.launch
```

This should open a Gazebo window, Rviz window and a teleop keyboard window. To see all the available topics that are being published check

```
rostopic list
```

As an example run

```
rostopic echo /model/semantic_rgbd_camera/pose 
```

This will show you the camera pose being updated over time. 

# Usage

You can write your own subscriber nodes using the pose and image information to project it to a (semantic point cloud). There is also a point cloud topic available which is generated by gazebo this does not include any semantic labels however (I used this point cloud as a reference to check if my projected semantic point cloud was scaled properly).

To move the camera I used the teleop keyboard which is publishing a twist message to the ros_bridge which is then transferring the message to the gazebo simulator which uses the tiwst message to move the camera. Alternatively it should be possible to control the camera by implenting a controller that publishes twist messages (I have not tested this). For instructions on how to use the teleop keyboard check the instructions given in the teleop keyboard terminal that opens.

I did change some coordinate frame references so there might be misalignments due to incorrect coordinate references. You can find some sample code in ./scripts/semantic_cloud_publisher.py which should technically work. But I had some trouble reading the semantic labels in another subscriber node from the published point cloud. I would suggest copying the intrinsic camera code and the time synchronization part. But the construction and orientation of the pointcloud istelves is not very stable/correct yet. I did not know how to fix it so maybe you have an idea. 

Also the camera_pose_transformer should work. But again might not be completeley corect anymore.

I ended up with a different script (not in this repo) that computed the 3D point coordinates and stored them in a numpy array with an additional column for the semantic label and used this to construct my voxelized maps directly from the numpy data without converting to a pointcloud first.